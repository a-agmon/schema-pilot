{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lancedb\n",
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install ipywidgets\n",
    "%pip install jupyter\n",
    "%pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "\n",
    "uri = \"runtime_assets/vecdb\"\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = db.open_table(\"somedb\")\n",
    "tbl.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.search(None).select([\"filename\"]).to_pandas().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.search(None).select([\"content\"]).to_pandas().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an embedding request to ollama  on the sentance \n",
    "sentence = \"where can I find data about allergy reactions of patients\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/embeddings\",\n",
    "        json={\"model\": \"nomic-embed-text\", \"prompt\": text}\n",
    "    )\n",
    "    return response.json()[\"embedding\"]\n",
    "\n",
    "question_embedding = get_embedding(sentence)\n",
    "# normalize the embedding to unit length\n",
    "question_embedding = question_embedding / np.linalg.norm(question_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# generate an embedding request to ollama  on the sentance \n",
    "sentence = \"which tables show data about blood tests\"\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "# 2. Generate embeddings for a list of sentences\n",
    "sentence_embeddings = model.encode(sentence)\n",
    "# 3. Normalize the embeddings to unit length\n",
    "sentence_embeddings = sentence_embeddings / np.linalg.norm(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tbl.search(sentence_embeddings, query_type=\"vector\") \\\n",
    "    .select([\"content\"]) \\\n",
    "    .limit(10).to_pandas()\n",
    "df.sort_values(by=\"content\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"content\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_tables = \"\\n\".join(df['content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an AI assistant that answers questions about database schemas and tables.\n",
    "Here are the relevant tables and columns:\n",
    "{rel_tables}\n",
    "\n",
    "Please answer the following question. First explain the purpose of the tables relevant to the question, and if needed provide a query to retrieve the data.\n",
    "Question:{sentence}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# send request to ollama\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# extract the SQL query from response\n",
    "sql_query = response.json()[\"response\"]\n",
    "print(\"Generated SQL query:\")\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
